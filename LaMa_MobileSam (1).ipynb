{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHwjitr3dpJz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import sys\n",
        "from google.colab import files\n",
        "\n",
        "os.makedirs(\"models_export\", exist_ok=True)\n",
        "output_dir = \"models_export\"\n",
        "\n",
        "print(\"üöÄ 1. Installing dependencies (2-3 min)...\")\n",
        "!pip install -q numpy==1.23.5 # Downgrade numpy for compatibility with older albumentations\n",
        "!pip install -q torch torchvision onnx onnxruntime timm onnxscript\n",
        "!pip install -q git+https://github.com/ChaoningZhang/MobileSAM.git\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install -q onnxruntime-gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.export # Required for torch.export.Dim\n",
        "\n",
        "# Define output_dir here to ensure it's available\n",
        "os.makedirs(\"models_export\", exist_ok=True)\n",
        "output_dir = \"models_export\"\n",
        "\n",
        "print(\"\\nüì± 2. Downloading and Converting MobileSAM...\")\n",
        "\n",
        "# Ensure MobileSAM library is installed right before import\n",
        "!pip install -q git+https://github.com/ChaoningZhang/MobileSAM.git\n",
        "# Ensure onnxscript is available for torch.onnx.export\n",
        "!pip install -q onnxscript\n",
        "\n",
        "from mobile_sam import sam_model_registry, SamPredictor\n",
        "\n",
        "if not os.path.exists(\"mobile_sam.pt\"):\n",
        "    !wget -q https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt\n",
        "\n",
        "device = \"cpu\"\n",
        "model_type = \"vit_t\"\n",
        "sam_checkpoint = \"mobile_sam.pt\"\n",
        "\n",
        "mobile_sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "mobile_sam.to(device=device)\n",
        "mobile_sam.eval()\n",
        "\n",
        "print(\"   -> Exporting Encoder (image processing)...\")\n",
        "\n",
        "class EncoderWrapper(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "encoder_wrapper = EncoderWrapper(mobile_sam.image_encoder)\n",
        "dummy_input = torch.randn(1, 3, 1024, 1024)\n",
        "\n",
        "torch.onnx.export(\n",
        "    encoder_wrapper,\n",
        "    dummy_input,\n",
        "    f\"{output_dir}/mobilesam_encoder.onnx\",\n",
        "    input_names=['x'], # Changed 'input' to 'x' to match forward method argument\n",
        "    output_names=['image_embeddings'],\n",
        "    dynamic_shapes={\n",
        "        'x': (1, 3, torch.export.Dim('height', min=8), torch.export.Dim('width', min=8))\n",
        "    },\n",
        "    opset_version=18, # Updated opset_version as per warning\n",
        "    do_constant_folding=True\n",
        ")\n",
        "\n",
        "print(\"   -> Exporting Decoder (mask generation)....\")\n",
        "\n",
        "class DecoderWrapper(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.mask_decoder = model.mask_decoder\n",
        "        self.prompt_encoder = model.prompt_encoder\n",
        "        self.image_encoder = model.image_encoder\n",
        "\n",
        "    def forward(self, image_embeddings, point_coords, point_labels):\n",
        "        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
        "            points=(point_coords, point_labels),\n",
        "            boxes=None,\n",
        "            masks=None,\n",
        "        )\n",
        "        low_res_masks, iou_predictions = self.mask_decoder(\n",
        "            image_embeddings=image_embeddings,\n",
        "            image_pe=self.prompt_encoder.get_dense_pe(),\n",
        "            sparse_prompt_embeddings=sparse_embeddings,\n",
        "            dense_prompt_embeddings=dense_embeddings,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "        return low_res_masks, iou_predictions\n",
        "\n",
        "decoder_wrapper = DecoderWrapper(mobile_sam)\n",
        "embed_dim = mobile_sam.image_encoder.neck[0].out_channels\n",
        "embed_size = 64\n",
        "\n",
        "dummy_embeddings = torch.randn(1, embed_dim, embed_size, embed_size)\n",
        "dummy_point_coords = torch.randn(1, 1, 2)\n",
        "dummy_point_labels = torch.randint(0, 4, (1, 1))\n",
        "\n",
        "torch.onnx.export(\n",
        "    decoder_wrapper,\n",
        "    (dummy_embeddings, dummy_point_coords, dummy_point_labels),\n",
        "    f\"{output_dir}/mobilesam_decoder.onnx\",\n",
        "    input_names=['image_embeddings', 'point_coords', 'point_labels'],\n",
        "    output_names=['masks', 'iou_predictions'],\n",
        "    dynamic_shapes={\n",
        "        'image_embeddings': (1, embed_dim, embed_size, embed_size), # Explicitly set batch to 1\n",
        "        'point_coords': (1, torch.export.Dim('num_points'), 2), # Explicitly set batch to 1, last dim is always 2\n",
        "        'point_labels': (1, torch.export.Dim('num_points')) # Explicitly set batch to 1\n",
        "    },\n",
        "    opset_version=18, # Updated opset_version as per warning\n",
        "    do_constant_folding=True\n",
        ")\n",
        "\n",
        "print(\"   -> Quantizing MobileSAM models...\")\n",
        "!pip install -q onnxruntime-gpu # Ensure onnxruntime is installed before import\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "# Quantize encoder\n",
        "quantize_dynamic(\n",
        "    model_input=f\"{output_dir}/mobilesam_encoder.onnx\",\n",
        "    model_output=f\"{output_dir}/mobilesam_encoder_quant.onnx\",\n",
        "    weight_type=QuantType.QUInt8\n",
        ")\n",
        "\n",
        "# Due to shape inference issues during quantization, keep decoder as FP32\n",
        "# quantize_dynamic(\n",
        "#     model_input=f\"{output_dir}/mobilesam_decoder.onnx\",\n",
        "#     model_output=f\"{output_dir}/mobilesam_decoder_quant.onnx\",\n",
        "#     weight_type=QuantType.QUInt8\n",
        "# )\n",
        "\n",
        "# os.remove(f\"{output_dir}/mobilesam_encoder.onnx\") # Keep FP32 version for now, might be useful\n",
        "# os.remove(f\"{output_dir}/mobilesam_decoder.onnx\") # Keep FP32 version for now, might be useful\n",
        "\n",
        "print(\"   -> Quantizing Encoder only (Decoder has shape conflicts)...\")\n",
        "print(\"   -> Simplifying Decoder (keeping FP32 for compatibility)...\")\n",
        "\n",
        "# Remove original FP32 encoder model as quantized version is available\n",
        "os.remove(f\"{output_dir}/mobilesam_encoder.onnx\")\n",
        "\n",
        "# Rename the FP32 decoder for clarity\n",
        "os.rename(f\"{output_dir}/mobilesam_decoder.onnx\", f\"{output_dir}/mobilesam_decoder_fp32.onnx\")\n",
        "\n",
        "print(\"   -> MobileSAM Ready!\")\n",
        "print(\"      Encoder: mobilesam_encoder_quant.onnx (quantized INT8)\")\n",
        "print(\"      Decoder: mobilesam_decoder_fp32.onnx (FP32 - better compatibility)\")"
      ],
      "metadata": {
        "id": "Zy7jO5cwd4gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38c32a19"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'lama')\n",
        "\n",
        "import yaml\n",
        "import torch.nn as nn\n",
        "from omegaconf import OmegaConf\n",
        "from saicinpainting.training.trainers.default import DefaultInpaintingTrainingModule\n",
        "\n",
        "print(\"   -> Loading PyTorch Model...\")\n",
        "\n",
        "with open('big-lama/config.yaml', 'r') as f:\n",
        "    train_config = OmegaConf.create(yaml.safe_load(f))\n",
        "\n",
        "checkpoint_path = 'big-lama/models/best.ckpt'\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "model = DefaultInpaintingTrainingTrainingModule(train_config)\n",
        "model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "model.eval()\n",
        "\n",
        "generator = model.generator\n",
        "generator.eval()\n",
        "\n",
        "class LaMaWrapper(nn.Module):\n",
        "    def __init__(self, generator):\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        batch = {'image': image, 'mask': mask}\n",
        "        return self.generator(batch)\n",
        "\n",
        "wrapper = LaMaWrapper(generator)\n",
        "print(\"   -> Model loaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fb4208a"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, 'lama')\n",
        "\n",
        "import yaml\n",
        "import torch.nn as nn\n",
        "from omegaconf import OmegaConf\n",
        "from saicinpainting.training.trainers.default import DefaultInpaintingTrainingModule\n",
        "\n",
        "print(\"   -> Loading PyTorch Model...\")\n",
        "\n",
        "with open('big-lama/config.yaml', 'r') as f:\n",
        "    train_config = OmegaConf.create(yaml.safe_load(f))\n",
        "\n",
        "checkpoint_path = 'big-lama/models/best.ckpt'\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "model = DefaultInpaintingTrainingModule(train_config)\n",
        "model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "model.eval()\n",
        "\n",
        "generator = model.generator\n",
        "generator.eval()\n",
        "\n",
        "class LaMaWrapper(nn.Module):\n",
        "    def __init__(self, generator):\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        batch = {'image': image, 'mask': mask}\n",
        "        return self.generator(batch)\n",
        "\n",
        "wrapper = LaMaWrapper(generator)\n",
        "print(\"   -> Model loaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n‚ù§\u0010 3. Baixando e Convertendo LaMa (Big-LaMa)...\")\n",
        "\n",
        "if not os.path.exists(\"lama\"):\n",
        "    !git clone -q https://github.com/advimman/lama.git\n",
        "\n",
        "print(\"   -> Installing LaMa dependencies...\")\n",
        "# Uninstall any existing albumentations to ensure version 0.4.6 is used\n",
        "!pip uninstall -y albumentations\n",
        "!pip install -q omegaconf hydra-core albumentations==0.4.6 kornia webdataset scikit-image opencv-python easydict\n",
        "!pip install -q wldhx.yadisk-direct pytorch-lightning\n",
        "!pip install -q webdataset\n",
        "\n",
        "if not os.path.exists(\"big-lama\"):\n",
        "    !curl -LJO https://huggingface.co/smartywu/big-lama/resolve/main/big-lama.zip\n",
        "    !unzip -q big-lama.zip -d big-lama"
      ],
      "metadata": {
        "id": "2ja2b3l5eA3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.insert(0, 'lama')\n",
        "\n",
        "import yaml\n",
        "import torch.nn as nn\n",
        "from omegaconf import OmegaConf\n",
        "from saicinpainting.training.trainers.default import DefaultInpaintingTrainingModule\n",
        "\n",
        "print(\"   -> Loading PyTorch Model...\")\n",
        "\n",
        "with open('big-lama/config.yaml', 'r') as f:\n",
        "    train_config = OmegaConf.create(yaml.safe_load(f))\n",
        "\n",
        "checkpoint_path = 'big-lama/models/best.ckpt'\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "model = DefaultInpaintingTrainingModule(train_config)\n",
        "model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "model.eval()\n",
        "\n",
        "generator = model.generator\n",
        "generator.eval()\n",
        "\n",
        "class LaMaWrapper(nn.Module):\n",
        "    def __init__(self, generator):\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        batch = {'image': image, 'mask': mask}\n",
        "        return self.generator(batch)\n",
        "\n",
        "wrapper = LaMaWrapper(generator)\n",
        "print(\"   -> Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "XX0v0T4zeJzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"   -> Converting to ONNX (FP32)...\")\n",
        "\n",
        "dummy_image = torch.randn(1, 3, 512, 512)\n",
        "dummy_mask = torch.randn(1, 1, 512, 512)\n",
        "onnx_path = f\"{output_dir}/lama.onnx\"\n",
        "\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(\n",
        "        wrapper,\n",
        "        (dummy_image, dummy_mask),\n",
        "        onnx_path,\n",
        "        input_names=['image', 'mask'],\n",
        "        output_names=['inpainted'],\n",
        "        dynamic_axes={\n",
        "            'image': {0: 'batch', 2: 'height', 3: 'width'},\n",
        "            'mask': {0: 'batch', 2: 'height', 3: 'width'},\n",
        "            'inpainted': {0: 'batch', 2: 'height', 3: 'width'}\n",
        "        },\n",
        "        opset_version=14,\n",
        "        do_constant_folding=True\n",
        "    )\n",
        "\n",
        "print(\"   -> Quantizing LaMa (INT8)...\")\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "quantize_dynamic(\n",
        "    model_input=onnx_path,\n",
        "    model_output=f\"{output_dir}/lama_quant.onnx\",\n",
        "    weight_type=QuantType.QUInt8\n",
        ")\n",
        "\n",
        "os.remove(onnx_path)\n",
        "print(\"   -> LaMa Ready!\")"
      ],
      "metadata": {
        "id": "4eHsackxD_sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüì¶ 4. Compressing and Downloading...\")\n",
        "!ls -lh {output_dir}/\n",
        "!zip -j models_postcard.zip {output_dir}/*\n",
        "files.download('models_postcard.zip')\n",
        "print(\"‚úÖ Complete! Copy files from ZIP to 'project/frontend/public/models/'\")"
      ],
      "metadata": {
        "id": "wZL-Yt80E-uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"LaMa_MobileSam.py\n",
        "\n",
        "Standalone script to export MobileSAM and LaMa models to ONNX.\n",
        "Compatible with Google Colab and local environments (Linux).\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import shutil\n",
        "import warnings\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Installs required dependencies if running in a notebook-like environment or requested.\"\"\"\n",
        "    print(\"üöÄ Installing dependencies...\")\n",
        "    commands = [\n",
        "        # Install numpy first, but we will enforce it again later\n",
        "        \"pip install -q \\\"numpy<2.0\\\"\",\n",
        "        \"pip install -q torch torchvision onnx onnxruntime timm onnxscript\",\n",
        "        \"pip install -q git+https://github.com/ChaoningZhang/MobileSAM.git\",\n",
        "        \"pip install -q git+https://github.com/facebookresearch/segment-anything.git\",\n",
        "        \"pip install -q omegaconf hydra-core albumentations==0.4.6 kornia webdataset scikit-image opencv-python easydict wldhx.yadisk-direct pytorch-lightning\",\n",
        "        # Force re-install albumentations to correct version\n",
        "        \"pip uninstall -y albumentations\",\n",
        "        \"pip install -q albumentations==0.4.6\",\n",
        "        # CRITICAL: Force numpy < 2.0 at the end to ensure no other package upgraded it\n",
        "        \"pip install -q \\\"numpy<2.0\\\"\"\n",
        "    ]\n",
        "    for cmd in commands:\n",
        "        print(f\"Executing: {cmd}\")\n",
        "        subprocess.check_call(cmd, shell=True)\n",
        "\n",
        "# Check args\n",
        "if len(sys.argv) > 1 and sys.argv[1] == \"--install\":\n",
        "    install_dependencies()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "try:\n",
        "    import mobile_sam\n",
        "    from mobile_sam import sam_model_registry\n",
        "except ImportError:\n",
        "    print(\"‚ùå MobileSAM not found. Run with --install to install dependencies.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Setup Output Directory\n",
        "os.makedirs(\"models_export\", exist_ok=True)\n",
        "output_dir = \"models_export\"\n",
        "\n",
        "def download_file(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"‚¨áÔ∏è Downloading {filename}...\")\n",
        "        subprocess.check_call(f\"wget -q -O {filename} {url}\", shell=True)\n",
        "    else:\n",
        "        print(f\"‚úÖ {filename} already exists.\")\n",
        "\n",
        "# ==========================================\n",
        "# 1. MobileSAM Export\n",
        "# ==========================================\n",
        "print(\"\\nüì± 1. Processing MobileSAM...\")\n",
        "\n",
        "download_file(\"https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt\", \"mobile_sam.pt\")\n",
        "\n",
        "device = \"cpu\"\n",
        "model_type = \"vit_t\"\n",
        "checkpoint = \"mobile_sam.pt\"\n",
        "\n",
        "print(f\"   -> Loading {model_type}...\")\n",
        "mobile_sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
        "mobile_sam_model.to(device=device)\n",
        "mobile_sam_model.eval()\n",
        "\n",
        "# --- Export Encoder ---\n",
        "print(\"   -> Exporting Encoder...\")\n",
        "\n",
        "class EncoderWrapper(nn.Module):\n",
        "    def __init__(self, encoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "encoder_wrapper = EncoderWrapper(mobile_sam_model.image_encoder)\n",
        "dummy_input = torch.randn(1, 3, 1024, 1024)\n",
        "\n",
        "encoder_onnx_path = os.path.join(output_dir, \"mobilesam_encoder.onnx\")\n",
        "\n",
        "# NOTE: Static shapes for Encoder (1x3x1024x1024)\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(\n",
        "        encoder_wrapper,\n",
        "        dummy_input,\n",
        "        encoder_onnx_path,\n",
        "        input_names=['x'],\n",
        "        output_names=['image_embeddings'],\n",
        "        opset_version=18, # Updated to 18 to match PyTorch defaults and avoid conversion errors\n",
        "        do_constant_folding=True\n",
        "    )\n",
        "\n",
        "# --- Export Decoder ---\n",
        "print(\"   -> Exporting Decoder...\")\n",
        "\n",
        "class DecoderWrapper(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.mask_decoder = model.mask_decoder\n",
        "        self.prompt_encoder = model.prompt_encoder\n",
        "\n",
        "    def forward(self, image_embeddings, point_coords, point_labels):\n",
        "        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
        "            points=(point_coords, point_labels),\n",
        "            boxes=None,\n",
        "            masks=None,\n",
        "        )\n",
        "        low_res_masks, iou_predictions = self.mask_decoder(\n",
        "            image_embeddings=image_embeddings,\n",
        "            image_pe=self.prompt_encoder.get_dense_pe(),\n",
        "            sparse_prompt_embeddings=sparse_embeddings,\n",
        "            dense_prompt_embeddings=dense_embeddings,\n",
        "            multimask_output=False,\n",
        "        )\n",
        "        return low_res_masks, iou_predictions\n",
        "\n",
        "decoder_wrapper = DecoderWrapper(mobile_sam_model)\n",
        "\n",
        "embed_dim = mobile_sam_model.image_encoder.neck[0].out_channels\n",
        "embed_size = 64\n",
        "\n",
        "dummy_embeddings = torch.randn(1, embed_dim, embed_size, embed_size)\n",
        "dummy_point_coords = torch.randn(1, 2, 2) # Fixed 2 points\n",
        "dummy_point_labels = torch.randn(1, 2)\n",
        "\n",
        "decoder_onnx_path = os.path.join(output_dir, \"mobilesam_decoder.onnx\")\n",
        "\n",
        "# NOTE: Static shapes for Decoder (1 image, 2 points)\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(\n",
        "        decoder_wrapper,\n",
        "        (dummy_embeddings, dummy_point_coords, dummy_point_labels),\n",
        "        decoder_onnx_path,\n",
        "        input_names=['image_embeddings', 'point_coords', 'point_labels'],\n",
        "        output_names=['masks', 'iou_predictions'],\n",
        "        opset_version=18, # Updated to 18\n",
        "        do_constant_folding=True\n",
        "    )\n",
        "\n",
        "# --- Quantization MobileSAM ---\n",
        "print(\"   -> Quantizing MobileSAM Encoder...\")\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "encoder_quant_path = os.path.join(output_dir, \"mobilesam_encoder_quant.onnx\")\n",
        "quantize_dynamic(\n",
        "    model_input=encoder_onnx_path,\n",
        "    model_output=encoder_quant_path,\n",
        "    weight_type=QuantType.QUInt8\n",
        ")\n",
        "\n",
        "# Clean up FP32 encoder\n",
        "if os.path.exists(encoder_onnx_path):\n",
        "    os.remove(encoder_onnx_path)\n",
        "\n",
        "# Rename Decoder (keep FP32)\n",
        "decoder_fp32_path = os.path.join(output_dir, \"mobilesam_decoder_fp32.onnx\")\n",
        "if os.path.exists(decoder_onnx_path):\n",
        "    # Rename/Move\n",
        "    if os.path.exists(decoder_fp32_path):\n",
        "        os.remove(decoder_fp32_path)\n",
        "    shutil.move(decoder_onnx_path, decoder_fp32_path)\n",
        "\n",
        "print(\"‚úÖ MobileSAM Export Complete.\")\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 2. LaMa Export\n",
        "# ==========================================\n",
        "print(\"\\nüé® 2. Processing LaMa...\")\n",
        "\n",
        "# CHECK NUMPY VERSION BEFORE IMPORTING LAMA\n",
        "import numpy as np\n",
        "print(f\"   -> Current NumPy version: {np.__version__}\")\n",
        "if int(np.__version__.split('.')[0]) >= 2:\n",
        "    print(\"‚ö†Ô∏è NumPy 2.x detected! This breaks LaMa.\")\n",
        "    print(\"   -> Downgrading NumPy to <2.0...\")\n",
        "    subprocess.check_call(\"pip install -q \\\"numpy<2.0\\\"\", shell=True)\n",
        "    # We need to reload numpy, but in a script that's hard.\n",
        "    # However, since we haven't imported LaMa modules yet,\n",
        "    # the new process spawned by imports might pick it up, or we might need a restart.\n",
        "    # But usually, 'import' caches the module.\n",
        "    # Let's hope the environment allows hot-swapping or user ran with --install.\n",
        "    print(\"   ‚ùó IF THIS FAILS, PLEASE RESTART RUNTIME AND RUN AGAIN ‚ùó\")\n",
        "\n",
        "if not os.path.exists(\"lama\"):\n",
        "    print(\"   -> Cloning LaMa...\")\n",
        "    subprocess.check_call(\"git clone -q https://github.com/advimman/lama.git\", shell=True)\n",
        "\n",
        "if not os.path.exists(\"big-lama\"):\n",
        "    print(\"   -> Downloading Big-LaMa weights...\")\n",
        "    subprocess.check_call(\"curl -LJO https://huggingface.co/smartywu/big-lama/resolve/main/big-lama.zip\", shell=True)\n",
        "    subprocess.check_call(\"unzip -q -o big-lama.zip -d big-lama\", shell=True)\n",
        "\n",
        "# Add lama to path\n",
        "sys.path.insert(0, os.path.abspath('lama'))\n",
        "\n",
        "try:\n",
        "    import yaml\n",
        "    from omegaconf import OmegaConf\n",
        "    from saicinpainting.training.trainers.default import DefaultInpaintingTrainingModule\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import LaMa modules: {e}\")\n",
        "    print(\"   This is usually due to NumPy 2.0 incompatibility.\")\n",
        "    print(\"   Try running: pip install \\\"numpy<2.0\\\" and RESTART THE RUNTIME.\")\n",
        "    sys.exit(1)\n",
        "except AttributeError as e:\n",
        "    print(f\"‚ùå Numpy Version Error: {e}\")\n",
        "    print(\"   You must downgrade numpy: pip install \\\"numpy<2.0\\\"\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"   -> Loading LaMa Model...\")\n",
        "# Handle nested folder structure if it occurs\n",
        "config_path = 'big-lama/config.yaml'\n",
        "checkpoint_path = 'big-lama/models/best.ckpt'\n",
        "\n",
        "if not os.path.exists(config_path):\n",
        "     if os.path.exists('big-lama/big-lama/config.yaml'):\n",
        "        config_path = 'big-lama/big-lama/config.yaml'\n",
        "        checkpoint_path = 'big-lama/big-lama/models/best.ckpt'\n",
        "\n",
        "with open(config_path, 'r') as f:\n",
        "    train_config = OmegaConf.create(yaml.safe_load(f))\n",
        "\n",
        "# Disable strict loading\n",
        "train_config.training_model.predict_only = True\n",
        "train_config.visualizer.kind = 'noop'\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "\n",
        "lama_model = DefaultInpaintingTrainingModule(train_config)\n",
        "lama_model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
        "lama_model.eval()\n",
        "\n",
        "class LaMaWrapper(nn.Module):\n",
        "    def __init__(self, generator):\n",
        "        super().__init__()\n",
        "        self.generator = generator\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        # LaMa generator expects a dict\n",
        "        batch = {'image': image, 'mask': mask}\n",
        "        return self.generator(batch)\n",
        "\n",
        "lama_wrapper = LaMaWrapper(lama_model.generator)\n",
        "\n",
        "dummy_image = torch.randn(1, 3, 512, 512)\n",
        "dummy_mask = torch.randn(1, 1, 512, 512)\n",
        "\n",
        "lama_onnx_path = os.path.join(output_dir, \"lama.onnx\")\n",
        "\n",
        "print(\"   -> Exporting LaMa to ONNX (FP32)...\")\n",
        "# User requested stability/quality over size (200MB is fine).\n",
        "# We will NOT quantize LaMa to avoid quality loss and potential INT8 operator issues.\n",
        "with torch.no_grad():\n",
        "    torch.onnx.export(\n",
        "        lama_wrapper,\n",
        "        (dummy_image, dummy_mask),\n",
        "        lama_onnx_path,\n",
        "        input_names=['image', 'mask'],\n",
        "        output_names=['inpainted'],\n",
        "        opset_version=18,\n",
        "        do_constant_folding=True\n",
        "    )\n",
        "\n",
        "# Skipping Quantization for LaMa as requested\n",
        "# lama_quant_path = os.path.join(output_dir, \"lama_quant.onnx\")\n",
        "# quantize_dynamic(...)\n",
        "\n",
        "print(\"‚úÖ LaMa Export Complete (FP32 - High Quality).\")\n",
        "\n",
        "# Compress\n",
        "print(\"\\nüì¶ Compressing models...\")\n",
        "zip_filename = \"models_postcard.zip\"\n",
        "shutil.make_archive(\"models_postcard\", 'zip', output_dir)\n",
        "print(f\"üéâ Done! Download {zip_filename}.zip and unzip to project/frontend/public/models/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HFN9llbHA0C5",
        "outputId": "ba65a8b6-cfbc-4024-d9ca-4339c8f75098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_5m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.12/dist-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_11m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.12/dist-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_224 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.12/dist-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_384 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.12/dist-packages/mobile_sam/modeling/tiny_vit_sam.py:656: UserWarning: Overwriting tiny_vit_21m_512 in registry with mobile_sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì± 1. Processing MobileSAM...\n",
            "‚úÖ mobile_sam.pt already exists.\n",
            "   -> Loading vit_t...\n",
            "   -> Exporting Encoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0130 13:19:34.624000 20039 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0130 13:19:34.626000 20039 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0130 13:19:34.630000 20039 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
            "W0130 13:19:34.632000 20039 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `EncoderWrapper([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `EncoderWrapper([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ‚úÖ\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n",
            "Applied 66 of general pattern rewrite rules.\n",
            "   -> Exporting Decoder...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0130 13:19:45.179000 20039 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0130 13:19:45.181000 20039 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0130 13:19:45.185000 20039 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
            "W0130 13:19:45.190000 20039 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `DecoderWrapper([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `DecoderWrapper([...]` with `torch.export.export(..., strict=False)`... ‚úÖ\n",
            "[torch.onnx] Run decomposition...\n",
            "[torch.onnx] Run decomposition... ‚úÖ\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ‚úÖ\n",
            "Applied 9 of general pattern rewrite rules.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   -> Quantizing MobileSAM Encoder...\n",
            "‚úÖ MobileSAM Export Complete.\n",
            "\n",
            "üé® 2. Processing LaMa...\n",
            "   -> Current NumPy version: 1.26.4\n",
            "Detectron v2 is not installed\n",
            "   -> Loading LaMa Model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint was not an allowed global by default. Please use `torch.serialization.add_safe_globals([pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint])` or the `torch.serialization.safe_globals([pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2693033184.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0mtrain_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'noop'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0mlama_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultInpaintingTrainingModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1527\u001b[0m                         )\n\u001b[1;32m   1528\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m                 return _load(\n\u001b[1;32m   1531\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint was not an allowed global by default. Please use `torch.serialization.add_safe_globals([pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint])` or the `torch.serialization.safe_globals([pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ]
    }
  ]
}